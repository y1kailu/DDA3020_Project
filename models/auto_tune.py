import optuna
import numpy as np
import pandas as pd
import os
import sys
import traceback # ç§»åˆ°æœ€ä¸Šé¢ï¼Œæ–¹ä¾¿å…¨å±€è°ƒç”¨
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import TimeSeriesSplit

# --- å¯¼å…¥æ¨¡å‹ ---
# ç¡®ä¿ä½ çš„ models æ–‡ä»¶å¤¹ä¸‹æœ‰è¿™äº›æ–‡ä»¶ï¼Œä¸” Ridge_Regression.py æ²¡æœ‰ç©ºæ ¼
try:
    from models import LGBM, XGboost, Ridge_Regression, MLP, LSTM
    from data.data_process import load_and_process_data
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿ï¼š\n1. data_process.py åœ¨ data/ ç›®å½•ä¸‹\n2. æ‰€æœ‰æ¨¡å‹æ–‡ä»¶åœ¨ models/ ç›®å½•ä¸‹\n3. Ridge Regression.py å·²é‡å‘½åä¸º Ridge_Regression.py")
    sys.exit(1)

# --- å…¨å±€é…ç½® ---
N_TRIALS_TREE = 30   # æ ‘æ¨¡å‹å°è¯• 30 æ¬¡
N_TRIALS_NN = 10     # ç¥ç»ç½‘ç»œå°è¯• 10 æ¬¡ (LSTM/MLP è¾ƒæ…¢)
DATA_PATH = 'data/train.csv'

def save_config(model_name, best_params):
    """å°†æœ€ä½³å‚æ•°å†™å…¥ configs/model_name_config.py"""
    os.makedirs('configs', exist_ok=True)
    file_path = f'configs/{model_name.lower()}_config.py'
    
    with open(file_path, 'w') as f:
        f.write(f"# Auto-generated config for {model_name}\n")
        f.write("params = {\n")
        for k, v in best_params.items():
            if isinstance(v, str):
                f.write(f"    '{k}': '{v}',\n")
            else:
                f.write(f"    '{k}': {v},\n")
        f.write("}\n")
    print(f"âœ… Saved optimized config to {file_path}")

def objective(trial, model_name, X, y, model_module):
    """
    ä½¿ç”¨ TimeSeriesSplit è¿›è¡Œäº¤å‰éªŒè¯è°ƒå‚
    """
    params = {}
    
    # === 1. å®šä¹‰æœç´¢ç©ºé—´ ===
    if model_name == 'LGBM':
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
            'num_leaves': trial.suggest_int('num_leaves', 20, 100),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'objective': 'regression',
            'metric': 'mse',
            'n_jobs': -1,
            'verbose': -1
        }
    elif model_name == 'XGBoost':
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'n_jobs': -1
        }
    elif model_name == 'Ridge':
        params = {
            'alpha': trial.suggest_float('alpha', 0.1, 100.0, log=True)
        }
    elif model_name == 'MLP':
        params = {
            'hidden_dim': trial.suggest_categorical('hidden_dim', [64, 128]),
            'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),
            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
            'batch_size': trial.suggest_categorical('batch_size', [64, 128]),
            'epochs': 10 
        }
    elif model_name == 'LSTM':
        params = {
            'hidden_dim': trial.suggest_categorical('hidden_dim', [32, 64]),
            'dropout': trial.suggest_float('dropout', 0.1, 0.4),
            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
            'batch_size': 128,
            'epochs': 5 # LSTM æ¯”è¾ƒæ…¢ï¼Œè°ƒå‚æ—¶ Epoch è®¾å°ä¸€ç‚¹
        }

    # === 2. äº¤å‰éªŒè¯ ===
    # 3æŠ˜äº¤å‰éªŒè¯
    tscv = TimeSeriesSplit(n_splits=3)
    cv_scores = []
    
    for train_idx, val_idx in tscv.split(X):
        X_train_cv, X_val_cv = X[train_idx], X[val_idx]
        y_train_cv, y_val_cv = y[train_idx], y[val_idx]
  
        try:
            # è°ƒç”¨æ¨¡å‹çš„ run å‡½æ•°
            preds = model_module.run(X_train_cv, y_train_cv, X_val_cv, params)
            mse = mean_squared_error(y_val_cv, preds)
            cv_scores.append(mse)
        except Exception as e:
            # å¦‚æœæŸç»„å‚æ•°å¯¼è‡´æ¨¡å‹å´©æºƒï¼ˆæ¯”å¦‚æ¢¯åº¦çˆ†ç‚¸ï¼‰ï¼Œè¿”å›æ— ç©·å¤§ï¼Œè®© Optuna è·³è¿‡
            print(f"âš ï¸ Error in trial: {e}")
            return float('inf')

    return np.mean(cv_scores)

def main():
    print("ğŸš€ Loading Data for Auto-Tuning...")
    # è¿™é‡Œä¼šä½¿ç”¨ data_process çš„ç¼“å­˜åŠŸèƒ½ï¼ˆå¦‚æœä¸Šæ¬¡è·‘è¿‡çš„è¯ï¼‰
    X, y = load_and_process_data(DATA_PATH)
    
    # === æ³¨å†Œæ‰€æœ‰ 5 ä¸ªæ¨¡å‹ ===
    models_map = {
        'Ridge': Ridge_Regression,
        'LGBM': LGBM,
        'XGBoost': XGboost,
        'MLP': MLP,
        'LSTM': LSTM  # <--- ä¹‹å‰è¿™é‡Œæ¼äº†ï¼ŒåŠ ä¸Šå®ƒï¼
    }
    
    for model_name, model_module in models_map.items():
        print(f"\n{'='*40}")
        print(f"ğŸ¤– Tuning {model_name}...")
        print(f"{'='*40}")
        
        # æœ€å°åŒ– MSE
        study = optuna.create_study(direction='minimize')
        
        # ç¥ç»ç½‘ç»œè·‘å¾—æ…¢ï¼Œæ¬¡æ•°è®¾å°‘ä¸€ç‚¹
        n_trials = N_TRIALS_NN if model_name in ['MLP', 'LSTM'] else N_TRIALS_TREE
        
        try:
            study.optimize(
                lambda trial: objective(trial, model_name, X, y, model_module), 
                n_trials=n_trials
            )
            
            print(f"ğŸ† Best MSE for {model_name}: {study.best_value:.6f}")
            print(f"ğŸ”§ Best Params: {study.best_params}")
            save_config(model_name, study.best_params)
            
        except Exception as e:
            print(f"âŒ Error tuning {model_name}: {e}")
            traceback.print_exc()

if __name__ == "__main__":
    main()