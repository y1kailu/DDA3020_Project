import optuna
import os
import numpy as np
from sklearn.metrics import mean_squared_error

# å¯¼å…¥æ•°æ®å¤„ç†
from data.data_process import load_and_process_data

# å¯¼å…¥ä½ çš„æ¨¡å‹æ¨¡å— (ç¡®ä¿å®ƒä»¬éƒ½åœ¨ models/ æ–‡ä»¶å¤¹ä¸‹ä¸”æœ‰ run å‡½æ•°)
from models import lgbm_model, xgboost_model, ridge_model, mlp_model, lstm_model

# å…¨å±€é…ç½®
N_TRIALS_TREE = 50   # æ ‘æ¨¡å‹å°è¯•æ¬¡æ•° (è·‘å¾—å¿«ï¼Œå¯ä»¥å¤šè¯•)
N_TRIALS_NN = 15     # ç¥ç»ç½‘ç»œå°è¯•æ¬¡æ•° (è·‘å¾—æ…¢ï¼Œå°‘è¯•å‡ æ¬¡)
DATA_PATH = 'data/train.csv'

def save_config(model_name, best_params):
    """å°†æœ€ä½³å‚æ•°å†™å…¥ configs/model_name_config.py"""
    os.makedirs('configs', exist_ok=True)
    file_path = f'configs/{model_name.lower()}_config.py'
    
    with open(file_path, 'w') as f:
        f.write(f"# Auto-generated config for {model_name}\n")
        f.write("params = {\n")
        for k, v in best_params.items():
            if isinstance(v, str):
                f.write(f"    '{k}': '{v}',\n")
            else:
                f.write(f"    '{k}': {v},\n")
        f.write("}\n")
    print(f"âœ… Saved optimized config to {file_path}")

def objective(trial, model_name, X_train, y_train, X_test, y_test):
    """å®šä¹‰ä¸åŒæ¨¡å‹çš„æœç´¢ç©ºé—´"""
    
    params = {}
    
    # === 1. LightGBM Search Space ===
    if model_name == 'LGBM':
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 500, 3000),
            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),
            'num_leaves': trial.suggest_int('num_leaves', 8, 64),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'objective': 'regression',
            'metric': 'mse',
            'random_state': 42,
            'n_jobs': -1,
            'verbose': -1
        }
        # è°ƒç”¨æ¨¡å‹è®­ç»ƒ (æ³¨æ„ï¼šä½ çš„ run å‡½æ•°éœ€è¦èƒ½æ¥æ”¶ params)
        y_pred = lgbm_model.run(X_train, y_train, X_test, params)

    # === 2. XGBoost Search Space ===
    elif model_name == 'XGBoost':
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 500, 3000),
            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'n_jobs': -1,
            'random_state': 42
        }
        y_pred = xgboost_model.run(X_train, y_train, X_test, params)

    # === 3. Ridge Regression Search Space ===
    elif model_name == 'Ridge':
        params = {
            'alpha': trial.suggest_float('alpha', 0.1, 1000.0, log=True)
        }
        y_pred = ridge_model.run(X_train, y_train, X_test, params)

    # === 4. MLP (ResNet) Search Space ===
    elif model_name == 'MLP':
        params = {
            'hidden_dim': trial.suggest_categorical('hidden_dim', [64, 128, 256]),
            'num_blocks': trial.suggest_int('num_blocks', 1, 4),
            'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),
            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
            'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]),
            'epochs': 20  # å›ºå®š Epochï¼Œé æ—©åœæˆ–å¿«é€ŸéªŒè¯
        }
        y_pred = mlp_model.run(X_train, y_train, X_test, params)
    
    # === 5. LSTM Search Space ===
    elif model_name == 'LSTM':
        params = {
            'hidden_dim': trial.suggest_categorical('hidden_dim', [32, 64, 128]),
            'num_layers': trial.suggest_int('num_layers', 1, 2),
            'dropout': trial.suggest_float('dropout', 0.1, 0.4),
            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
            'batch_size': 128,
            'epochs': 15 
        }
        y_pred = lstm_model.run(X_train, y_train, X_test, params)

    # è®¡ç®— MSE (è¿™é‡Œä¸ç”¨ Sharpe æ˜¯å› ä¸º Sharpe å¾ˆéš¾ç›´æ¥ä¼˜åŒ–ï¼ŒMSE ç¨³å¥)
    # å¦‚æœ y_pred æ˜¯ tensor æˆ– listï¼Œè½¬ä¸º numpy
    if not isinstance(y_pred, np.ndarray):
        y_pred = np.array(y_pred)
    
    # ç¡®ä¿ç»´åº¦åŒ¹é…
    y_pred = y_pred.flatten()
    
    mse = mean_squared_error(y_test, y_pred)
    return mse

def main():
    print("ğŸš€ Loading Data for Auto-Tuning...")
    # åªéœ€è¦åŠ è½½ä¸€æ¬¡æ•°æ®
    X_train, X_test, y_train, y_test = load_and_process_data(DATA_PATH)
    
    # å®šä¹‰è¦ä¼˜åŒ–çš„æ¨¡å‹
    models_to_tune = ['Ridge', 'LGBM', 'XGBoost', 'MLP', 'LSTM']
    
    for model_name in models_to_tune:
        print(f"\n===========================================")
        print(f"ğŸ¤– Tuning {model_name}...")
        print(f"===========================================")
        
        # å®šä¹‰ä¼˜åŒ–æ–¹å‘ (minimize MSE)
        study = optuna.create_study(direction='minimize')
        
        # é’ˆå¯¹ä¸åŒæ¨¡å‹è®¾ç½®ä¸åŒçš„ Trial æ¬¡æ•°
        n_trials = N_TRIALS_NN if model_name in ['MLP', 'LSTM'] else N_TRIALS_TREE
        
        # å¼€å§‹ä¼˜åŒ–
        try:
            study.optimize(
                lambda trial: objective(trial, model_name, X_train, y_train, X_test, y_test), 
                n_trials=n_trials
            )
            
            print(f"ğŸ† Best MSE for {model_name}: {study.best_value:.6f}")
            print(f"ğŸ”§ Best Params: {study.best_params}")
            
            # è‡ªåŠ¨ä¿å­˜åˆ° configs/
            save_config(model_name, study.best_params)
            
        except Exception as e:
            print(f"âŒ Error tuning {model_name}: {e}")
            print("Skipping to next model...")

if __name__ == "__main__":
    main()