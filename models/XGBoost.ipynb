{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"},{"sourceId":13825291,"sourceType":"datasetVersion","datasetId":8804496}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport xgboost as xgb\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass FastXGBPredictor:\n    def __init__(self):\n        self.model = None\n        self.feature_groups = {}\n        self.best_params = None\n        \n    def load_cleaned_data(self, file_path):\n        \"\"\"加载清洗好的数据\"\"\"\n        print(\"Loading cleaned data...\")\n        df = pd.read_csv(file_path)\n        print(f\"Data shape: {df.shape}\")\n        return df\n    \n    def create_feature_groups_by_importance(self, train_df, target_col='forward_returns', top_pct=0.1, mid_pct=0.5):\n        \"\"\"基于特征重要性创建三组特征\"\"\"\n        print(\"Creating feature groups by importance...\")\n        \n        # 准备特征和目标\n        feature_cols = [col for col in train_df.columns if col not in \n                       ['date_id', 'forward_returns', 'risk_free_rate', \n                        'market_forward_excess_returns', 'is_scored']]\n        \n        X = train_df[feature_cols].fillna(0)\n        y = train_df[target_col]\n        \n        # 训练初始模型获取特征重要性\n        initial_model = xgb.XGBRegressor(\n            n_estimators=100,\n            max_depth=4,\n            learning_rate=0.1,\n            random_state=42\n        )\n        initial_model.fit(X, y)\n        \n        # 获取特征重要性\n        importance_df = pd.DataFrame({\n            'feature': feature_cols,\n            'importance': initial_model.feature_importances_\n        }).sort_values('importance', ascending=False)\n        \n        # 创建特征组\n        n_features = len(importance_df)\n        n_top = int(n_features * top_pct)\n        n_mid = int(n_features * mid_pct) - n_top\n        \n        top_features = importance_df.head(n_top)['feature'].tolist()\n        mid_features = importance_df.iloc[n_top:n_top+n_mid]['feature'].tolist()\n        low_features = importance_df.iloc[n_top+n_mid:]['feature'].tolist()\n        \n        self.feature_groups = {\n            'top_10pct': top_features,\n            'mid_10_50pct': mid_features,\n            'low_50pct': low_features\n        }\n        \n        print(f\"Top {len(top_features)} features (前10%)\")\n        print(f\"Mid {len(mid_features)} features (前10-50%)\") \n        print(f\"Low {len(low_features)} features (剩余50%)\")\n        \n        return self.feature_groups\n    \n    def tune_regularization_params(self, X_train, y_train, feature_group_name, features):\n        \"\"\"调试单个特征组的正则化参数\"\"\"\n        print(f\"\\nTuning {feature_group_name} with {len(features)} features...\")\n        \n        X_subset = X_train[features].fillna(0)\n        \n        # 正则化参数网格\n        param_combinations = []\n        for reg_alpha in [0, 0.1, 0.5, 1, 2]:  # L1正则化\n            for reg_lambda in [0.1, 0.5, 1, 2, 5]:  # L2正则化\n                for max_depth in [3, 4, 5]:\n                    param_combinations.append({\n                        'reg_alpha': reg_alpha,\n                        'reg_lambda': reg_lambda,\n                        'max_depth': max_depth\n                    })\n        \n        # 时间序列交叉验证\n        tscv = TimeSeriesSplit(n_splits=3)\n        best_score = float('inf')\n        best_params = None\n        \n        # 测试前10个参数组合（为了速度）\n        for i, params in enumerate(param_combinations[:10]):\n            try:\n                model = xgb.XGBRegressor(\n                    n_estimators=100,\n                    learning_rate=0.1,\n                    reg_alpha=params['reg_alpha'],\n                    reg_lambda=params['reg_lambda'],\n                    max_depth=params['max_depth'],\n                    random_state=42,\n                    n_jobs=-1\n                )\n                \n                # 交叉验证\n                cv_scores = []\n                for train_idx, val_idx in tscv.split(X_subset):\n                    X_tr, X_val = X_subset.iloc[train_idx], X_subset.iloc[val_idx]\n                    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n                    \n                    model.fit(X_tr, y_tr)\n                    preds = model.predict(X_val)\n                    score = mean_squared_error(y_val, preds)\n                    cv_scores.append(score)\n                \n                avg_score = np.mean(cv_scores)\n                \n                if avg_score < best_score:\n                    best_score = avg_score\n                    best_params = params\n                    best_params['score'] = avg_score\n                    \n            except Exception as e:\n                continue\n        \n        return best_params\n    \n    def find_best_regularization_combinations(self, train_df, target_col='forward_returns'):\n        \"\"\"为三组特征找出最好的三个正则化参数组合\"\"\"\n        print(\"Finding best regularization combinations...\")\n        \n        X_train = train_df\n        y_train = train_df[target_col]\n        \n        # 为每组特征调试参数\n        group_results = {}\n        for group_name, features in self.feature_groups.items():\n            best_params = self.tune_regularization_params(X_train, y_train, group_name, features)\n            if best_params:\n                group_results[group_name] = best_params\n                print(f\"{group_name}: {best_params}\")\n        \n        # 选择表现最好的三个组合\n        all_combinations = []\n        for group_name, params in group_results.items():\n            params_copy = params.copy()\n            params_copy['feature_group'] = group_name\n            all_combinations.append(params_copy)\n        \n        # 按分数排序，选择最好的三个\n        all_combinations.sort(key=lambda x: x['score'])\n        best_combinations = all_combinations[:3]\n        \n        print(f\"\\nBest 3 regularization combinations:\")\n        for i, combo in enumerate(best_combinations, 1):\n            print(f\"{i}. {combo}\")\n        \n        self.best_params = best_combinations\n        return best_combinations\n    \n    def train_final_model(self, train_df, test_df, target_col='forward_returns'):\n        \"\"\"使用最好的参数组合训练最终模型\"\"\"\n        print(\"\\nTraining final model with best parameters...\")\n        \n        # 选择最好的特征组\n        best_group = self.best_params[0]['feature_group']\n        features = self.feature_groups[best_group]\n        params = self.best_params[0]\n        \n        print(f\"Using feature group: {best_group} with {len(features)} features\")\n        print(f\"Parameters: {params}\")\n        \n        X_train = train_df[features].fillna(0)\n        y_train = train_df[target_col]\n        X_test = test_df[features].fillna(0)\n        \n        # 训练最终模型\n        self.model = xgb.XGBRegressor(\n            n_estimators=200,\n            learning_rate=0.1,\n            reg_alpha=params['reg_alpha'],\n            reg_lambda=params['reg_lambda'],\n            max_depth=params['max_depth'],\n            random_state=42,\n            n_jobs=-1\n        )\n        \n        self.model.fit(X_train, y_train)\n        \n        # 预测\n        train_pred = self.model.predict(X_train)\n        test_pred = self.model.predict(X_test)\n        \n        # 计算训练集RMSE\n        train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n        print(f\"Training RMSE: {train_rmse:.6f}\")\n        \n        return test_pred\n    \n    def calculate_positions(self, predictions, method='adaptive_sigmoid'):\n        \"\"\"计算仓位权重 (0-2)\"\"\"\n        if method == 'adaptive_sigmoid':\n            # 自适应sigmoid，根据预测值的分布调整\n            pred_mean = np.mean(predictions)\n            pred_std = np.std(predictions)\n            \n            # 动态调整缩放因子\n            if pred_std > 0:\n                scale_factor = 1 / (pred_std * 2)\n            else:\n                scale_factor = 10\n                \n            # 使用sigmoid函数映射到0-2范围\n            positions = 1 + (1 / (1 + np.exp(-(predictions - pred_mean) * scale_factor)) - 0.5) * 2\n        \n        elif method == 'volatility_adjusted':\n            # 波动率调整方法\n            pred_vol = np.std(predictions)\n            if pred_vol > 0:\n                # 标准化预测值\n                z_scores = (predictions - np.mean(predictions)) / pred_vol\n                # 根据z-score确定仓位\n                positions = np.clip(1 + z_scores * 0.5, 0, 2)\n            else:\n                positions = np.ones_like(predictions)\n        \n        else:\n            # 简单截断方法\n            positions = np.clip(predictions * 5 + 1, 0, 2)\n        \n        # 确保在[0,2]范围内\n        positions = np.clip(positions, 0, 2)\n        \n        print(f\"Position stats - Min: {positions.min():.3f}, Max: {positions.max():.3f}, Mean: {positions.mean():.3f}\")\n        return positions\n\ndef main():\n    \"\"\"主函数\"\"\"\n    predictor = FastXGBPredictor()\n    \n    # 加载数据\n    train_df = predictor.load_cleaned_data('/kaggle/input/cleaned/train_cleaned.csv')\n    test_df = predictor.load_cleaned_data('/kaggle/input/hull-tactical-market-prediction/test.csv')  # 假设test数据格式相同\n    \n    # 创建特征分组\n    feature_groups = predictor.create_feature_groups_by_importance(train_df)\n    \n    # 找出最好的正则化参数组合\n    best_combinations = predictor.find_best_regularization_combinations(train_df)\n    \n    # 训练最终模型并预测\n    test_predictions = predictor.train_final_model(train_df, test_df)\n    \n    # 计算仓位\n    positions = predictor.calculate_positions(test_predictions, method='adaptive_sigmoid')\n    \n    # 创建提交文件\n    submission = pd.DataFrame({\n        'date_id': test_df['date_id'],\n        'weight': positions\n    })\n    \n    # 保存结果\n    submission.to_csv('submission.parquet', index=False)\n    print(f\"\\nSubmission saved with {len(submission)} predictions\")\n    \n    # 显示仓位分布\n    print(\"\\nPosition distribution:\")\n    print(submission['weight'].describe())\n    \n    return predictor, submission\n\n\n\nif __name__ == \"__main__\":\n    # 完整版本\n    predictor, submission = main()\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T09:48:44.316975Z","iopub.execute_input":"2025-11-23T09:48:44.317244Z","iopub.status.idle":"2025-11-23T09:49:05.270845Z","shell.execute_reply.started":"2025-11-23T09:48:44.317223Z","shell.execute_reply":"2025-11-23T09:49:05.270044Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cleaned/train_cleaned.csv\n/kaggle/input/hull-tactical-market-prediction/train.csv\n/kaggle/input/hull-tactical-market-prediction/test.csv\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_inference_server.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_gateway.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/__init__.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/templates.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/base_gateway.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/relay.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/kaggle_evaluation.proto\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/__init__.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/__init__.py\nLoading cleaned data...\nData shape: (9021, 90)\nLoading cleaned data...\nData shape: (10, 99)\nCreating feature groups by importance...\nTop 8 features (前10%)\nMid 35 features (前10-50%)\nLow 43 features (剩余50%)\nFinding best regularization combinations...\n\nTuning top_10pct with 8 features...\ntop_10pct: {'reg_alpha': 0, 'reg_lambda': 2, 'max_depth': 3, 'score': 0.00014185957904091346}\n\nTuning mid_10_50pct with 35 features...\nmid_10_50pct: {'reg_alpha': 0, 'reg_lambda': 2, 'max_depth': 3, 'score': 0.00015457243060023717}\n\nTuning low_50pct with 43 features...\nlow_50pct: {'reg_alpha': 0, 'reg_lambda': 2, 'max_depth': 3, 'score': 0.0001452557409445819}\n\nBest 3 regularization combinations:\n1. {'reg_alpha': 0, 'reg_lambda': 2, 'max_depth': 3, 'score': 0.00014185957904091346, 'feature_group': 'top_10pct'}\n2. {'reg_alpha': 0, 'reg_lambda': 2, 'max_depth': 3, 'score': 0.0001452557409445819, 'feature_group': 'low_50pct'}\n3. {'reg_alpha': 0, 'reg_lambda': 2, 'max_depth': 3, 'score': 0.00015457243060023717, 'feature_group': 'mid_10_50pct'}\n\nTraining final model with best parameters...\nUsing feature group: top_10pct with 8 features\nParameters: {'reg_alpha': 0, 'reg_lambda': 2, 'max_depth': 3, 'score': 0.00014185957904091346, 'feature_group': 'top_10pct'}\nTraining RMSE: 0.009779\nPosition stats - Min: 0.770, Max: 1.593, Mean: 0.992\n\nSubmission saved with 10 predictions\n\nPosition distribution:\ncount    10.000000\nmean      0.991956\nstd       0.237138\nmin       0.769819\n25%       0.884121\n50%       0.926410\n75%       1.047807\nmax       1.593434\nName: weight, dtype: float64\n","output_type":"stream"}],"execution_count":1}]}