import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
import os
import sys
import importlib

# --- 1. å¯¼å…¥ä½ çš„æ¨¡å— (å…¼å®¹ä¸åŒæ–‡ä»¶å¤¹ç»“æ„) ---
try:
    # å°è¯•ä» data æ–‡ä»¶å¤¹å’Œ models æ–‡ä»¶å¤¹å¯¼å…¥
    from data.data_process import load_and_process_data
    from models import LGBM, XGboost, Ridge_Regression, MLP, LSTM
except ImportError:
    # å¦‚æœæ‰€æœ‰æ–‡ä»¶éƒ½åœ¨æ ¹ç›®å½•
    from data_process import load_and_process_data
    import LGBM, XGboost, Ridge_Regression, MLP, LSTM

# --- å…¨å±€ç»˜å›¾è®¾ç½® ---
sns.set(style="whitegrid")
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans'] # å°è¯•æ˜¾ç¤ºä¸­æ–‡ï¼Œå¦‚æœä¸è¡Œå›é€€åˆ°è‹±æ–‡
plt.rcParams['axes.unicode_minus'] = False 

DATA_PATH = 'data/train.csv'

def load_params(model_name):
    """
    å°è¯•ä» configs/ æ–‡ä»¶å¤¹åŠ è½½ auto_tune è·‘å‡ºæ¥çš„æœ€ä½³å‚æ•°ã€‚
    å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œè¿”å›ç©ºå­—å…¸ï¼ˆä½¿ç”¨æ¨¡å‹é»˜è®¤å‚æ•°ï¼‰ã€‚
    """
    config_path = f"configs.{model_name.lower()}_config"
    try:
        if config_path in sys.modules:
            return sys.modules[config_path].params
        
        mod = importlib.import_module(config_path)
        print(f"âœ… Loaded tuned params for {model_name}")
        return mod.params
    except ImportError:
        print(f"âš ï¸ No config found for {model_name}, using default params.")
        return {}

def plot_results(model_name, y_true, y_pred, r2_score_val):
    """
    ç”»å›¾å‡½æ•°ï¼šå·¦è¾¹æ˜¯æŠ˜çº¿å¯¹æ¯”ï¼ˆåªç”»æœ€å150ä¸ªç‚¹ï¼‰ï¼Œå³è¾¹æ˜¯æ•£ç‚¹å›¾
    """
    plt.figure(figsize=(16, 6))
    
    # --- å­å›¾ 1: æŠ˜çº¿å›¾ (Line Plot) ---
    plt.subplot(1, 2, 1)
    # åªå–æœ€å 150 ä¸ªç‚¹ï¼Œè®©å›¾è¡¨æ¸…æ™°å¯è§
    subset_n = 150
    if len(y_true) > subset_n:
        y_true_plot = y_true[-subset_n:]
        y_pred_plot = y_pred[-subset_n:]
    else:
        y_true_plot = y_true
        y_pred_plot = y_pred
        
    plt.plot(range(len(y_true_plot)), y_true_plot, label='Actual (Truth)', color='black', alpha=0.7, linewidth=1.5)
    plt.plot(range(len(y_pred_plot)), y_pred_plot, label='Predicted', color='#ff4b4b', linestyle='--', linewidth=1.5)
    plt.title(f'{model_name}: Actual vs Predicted (Last {subset_n} Time Steps)', fontsize=14)
    plt.xlabel('Time Steps', fontsize=12)
    plt.ylabel('Returns', fontsize=12)
    plt.legend(fontsize=12)
    
    # --- å­å›¾ 2: æ•£ç‚¹å›¾ (Scatter Plot) ---
    plt.subplot(1, 2, 2)
    plt.scatter(y_true, y_pred, alpha=0.5, s=10, color='#1f77b4', label='Data Points')
    
    # ç”»å¯¹è§’çº¿ y=x (å®Œç¾é¢„æµ‹çº¿)
    min_val = min(y_true.min(), y_pred.min())
    max_val = max(y_true.max(), y_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Fit (y=x)')
    
    plt.title(f'{model_name}: Scatter Plot ($R^2$ = {r2_score_val:.4f})', fontsize=14)
    plt.xlabel('Actual Values', fontsize=12)
    plt.ylabel('Predicted Values', fontsize=12)
    plt.legend(fontsize=12)
    
    plt.tight_layout()
    # ä¿å­˜å›¾ç‰‡
    os.makedirs('results', exist_ok=True)
    save_path = f'results/{model_name}_performance.png'
    plt.savefig(save_path, dpi=150)
    print(f"   ğŸ“Š Plot saved to {save_path}")
    plt.close() # å…³é—­ç”»å¸ƒï¼Œé‡Šæ”¾å†…å­˜

def main():
    print("ğŸš€ Starting Main Pipeline with Cross-Validation...")
    
    # 1. åŠ è½½æ•°æ®
    X, y = load_and_process_data(DATA_PATH)
    
    # 2. å®šä¹‰è¦è¿è¡Œçš„æ¨¡å‹åˆ—è¡¨
    # ç¡®ä¿è¿™é‡Œçš„åå­—å’Œ keys ä¸ auto_tune ç”Ÿæˆçš„ config åå­—ä¸€è‡´
    models_map = {
        'Ridge': Ridge_Regression,
        'LGBM': LGBM,
        'XGBoost': XGboost,
        'MLP': MLP,
        'LSTM': LSTM
    }
    
    # 3. è®¾ç½®äº¤å‰éªŒè¯ (Time Series Split)
    n_splits = 5
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    final_metrics = []
    
    print(f"\n{'='*60}")
    print(f"ğŸ§ª Running {n_splits}-Fold Time Series Cross-Validation")
    print(f"{'='*60}\n")
    
    for name, model_module in models_map.items():
        print(f"â–¶ï¸  Running Model: {name}")
        
        # åŠ è½½å‚æ•°
        params = load_params(name)
        
        cv_rmse = []
        cv_r2 = []
        
        # ç”¨äºä¿å­˜æœ€åä¸€æŠ˜çš„æ•°æ®æ¥ç”»å›¾
        last_fold_y_true = None
        last_fold_y_pred = None
        
        fold = 1
        for train_index, test_index in tscv.split(X):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]
            
            try:
                # è¿è¡Œæ¨¡å‹ (ä½¿ç”¨ç»Ÿä¸€æ¥å£ run)
                y_pred = model_module.run(X_train, y_train, X_test, params)
                
                # è®¡ç®—æŒ‡æ ‡
                mse = mean_squared_error(y_test, y_pred)
                rmse = np.sqrt(mse)
                r2 = r2_score(y_test, y_pred)
                
                cv_rmse.append(rmse)
                cv_r2.append(r2)
                
                # å¦‚æœæ˜¯æœ€åä¸€æŠ˜ï¼Œä¿å­˜æ•°æ®ç”¨äºç”»å›¾
                if fold == n_splits:
                    last_fold_y_true = y_test
                    last_fold_y_pred = y_pred
                
                # print(f"   Fold {fold}/{n_splits}: RMSE={rmse:.5f}, R2={r2:.5f}")
                fold += 1
                
            except Exception as e:
                print(f"   âŒ Error in Fold {fold}: {e}")
                # é‡åˆ°é”™è¯¯å¡«å…¥ NaNï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ
                cv_rmse.append(np.nan)
                cv_r2.append(np.nan)

        # æ±‡æ€»å½“å‰æ¨¡å‹ç»“æœ
        avg_rmse = np.nanmean(cv_rmse)
        avg_r2 = np.nanmean(cv_r2)
        
        print(f"   âœ… Average RMSE: {avg_rmse:.5f} | Average R2: {avg_r2:.5f}")
        
        final_metrics.append({
            'Model': name,
            'CV RMSE': avg_rmse,
            'CV R2': avg_r2
        })
        
        # ç”»å›¾ (åªç”»æœ€åä¸€æŠ˜çš„è¡¨ç°)
        if last_fold_y_true is not None:
            plot_results(name, last_fold_y_true, last_fold_y_pred, avg_r2)
        
        print("-" * 40)

    # --- 4. è¾“å‡ºæœ€ç»ˆè¡¨æ ¼ ---
    print(f"\n{'='*20} ğŸ† Final Results Summary ğŸ† {'='*20}")
    results_df = pd.DataFrame(final_metrics)
    
    # æŒ‰ç…§ R2 æ’åº (R2 è¶Šé«˜è¶Šå¥½)
    results_df = results_df.sort_values(by='CV R2', ascending=False)
    
    print(results_df)
    
    # ä¿å­˜è¡¨æ ¼
    results_df.to_csv('final_model_comparison.csv', index=False)
    print("\nâœ… Results saved to 'final_model_comparison.csv'")
    print("âœ… Plots saved in 'results/' folder")

if __name__ == "__main__":
    main()